\documentclass[12pt]{article}
\usepackage[a4paper, margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{float}
\usepackage{fancyhdr}
\setlength{\headheight}{14.5pt}
\addtolength{\topmargin}{-2.5pt}
\usepackage{titlesec}
\usepackage{color}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolor}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolor},   
    commentstyle=\color{codegray},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{red},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,                 
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\pagestyle{fancy}
\fancyhf{}
\rhead{Machine Learning Report}
\lhead{Final Project}
\rfoot{Page \thepage}

\title{Final Project Report: Classification Algorithms in Machine Learning}
\author{Mihavana RAHOLDINA FIARA}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report presents an end-to-end machine learning project focused on classification tasks using real-world weather data. The goal is to preprocess the dataset, explore and visualize important features, apply a variety of classification models, and compare their performance.
\end{abstract}
\newpage

\tableofcontents
\newpage

\section{Introduction}

\hspace*{1cm}In this final machine learning project, we explore the application of various supervised learning algorithms to a real-world weather dataset. The primary goal is to predict whether it will rain the next day based on a range of meteorological features such as temperature, humidity, wind speed, and atmospheric pressure. This kind of predictive modeling can be particularly useful for early warning systems and decision-making in weather-sensitive industries.

The project follows a standard machine learning pipeline that includes several key stages: data loading, preprocessing, exploratory data analysis (EDA), model selection, training, and performance evaluation. Throughout the process, we implement and compare several well-known classification algorithms including Linear Regression, Logistic Regression, Decision Tree, K-Nearest Neighbors (KNN), and Support Vector Machines (SVM). Each algorithm is evaluated using appropriate classification metrics to assess accuracy, robustness, and generalization capabilities.

Additionally, we place emphasis on data quality, feature engineering, and the interpretability of results, which are critical in real-world deployment scenarios. By systematically comparing the strengths and limitations of each model, this project aims to identify the most effective approach for this specific prediction task.
\newpage

\section{Dataset Overview}
The dataset used in this project originates from the Australian Government's Bureau of Meteorology. It contains daily weather observations from 2008 to 2017. The objective is to predict whether it will rain the next day based on historical features.

\begin{table}[H]
\centering
\small
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{| l | p{7cm} | l | l |}
\hline
\textbf{Field} & \textbf{Description} & \textbf{Unit} & \textbf{Type} \\
\hline
Date & Date of the Observation in YYYY-MM-DD & Date & object \\
Location & Location of the Observation & Location & object \\
MinTemp & Minimum temperature & Celsius & float \\
MaxTemp & Maximum temperature & Celsius & float \\
Rainfall & Amount of rainfall & Millimeters & float \\
Evaporation & Amount of evaporation & Millimeters & float \\
Sunshine & Amount of bright sunshine & Hours & float \\
WindGustDir & Direction of the strongest gust & Compass Points & object \\
WindGustSpeed & Speed of the strongest gust & Kilometers/Hour & object \\
WindDir9am & Wind direction averaged of 10 minutes prior to 9am & Compass Points & object \\
WindDir3pm & Wind direction averaged of 10 minutes prior to 3pm & Compass Points & object \\
WindSpeed9am & Wind speed averaged of 10 minutes prior to 9am & Kilometers/Hour & float \\
WindSpeed3pm & Wind speed averaged of 10 minutes prior to 3pm & Kilometers/Hour & float \\
Humidity9am & Humidity at 9am & Percent & float \\
Humidity3pm & Humidity at 3pm & Percent & float \\
Pressure9am & Atmospheric pressure reduced to mean sea level at 9am & Hectopascal & float \\
Pressure3pm & Atmospheric pressure reduced to mean sea level at 3pm & Hectopascal & float \\
Cloud9am & Fraction of the sky obscured by cloud at 9am & Eights & float \\
Cloud3pm & Fraction of the sky obscured by cloud at 3pm & Eights & float \\
Temp9am & Temperature at 9am & Celsius & float \\
Temp3pm & Temperature at 3pm & Celsius & float \\
RainToday & If there was rain today & Yes/No & object \\
RISK\_MM & Amount of rain tomorrow & Millimeters & float \\
RainTomorrow & If there is rain tomorrow & Yes/No & float \\
\hline
\end{tabular}
\caption{Description of dataset fields}
\label{tab:dataset_description}
\end{table}
\newpage

\subsection{Features Description}
The dataset includes various meteorological features such as temperature, humidity, wind speed, and rainfall statistics. The target variable is \texttt{RainTomorrow}, which is a binary classification label.

\section{Data Preprocessing}
Refer to Appendix~\ref{appendix:preprocessing}, data preprocessing involved several steps:
\begin{itemize}
    \item Handling missing values
    \item Encoding categorical features
    \item Feature scaling
\end{itemize}

\section{Exploratory Data Analysis}
EDA was conducted to better understand the distribution of features and their relationships with the target variable. Plots and summary statistics helped to identify patterns and potential predictors.

\section{Modeling Approach}
We applied and compared the following classification algorithms:
\begin{itemize}
    \item Linear Regression
    \item Logistic Regression
    \item Decision Tree
    \item K-Nearest Neighbors (KNN)
    \item Support Vector Machines (SVM)
\end{itemize}

\subsection{Linear Regression}
Linear Regression is a supervised learning algorithm used for predicting a continuous output variable based on one or more input features. It assumes a linear relationship between the input variables and the output. The model finds the best-fit line by minimizing the sum of squared residuals between predicted and actual values.

\textbf{Example use case:} Predicting housing prices based on features like size, location, and number of bedrooms.

Refer to Appendix~\ref{appendix:LinearReg}, linear regression is evaluated using appropriate metrics which are MAE, MSE and R².

\subsubsection{MAE – Mean Absolute Error}
Mean Absolute Error (MAE) measures the average magnitude of the errors between predicted values and actual values, without considering their direction. It is calculated as the average of the absolute differences between predictions and true values. MAE gives a straightforward interpretation of the average prediction error in the same units as the target variable.

\[
\text{MAE} = \frac{1}{n} \sum_{i=1}^n | y_i - \hat{y}_i |
\]

where $y_i$ is the actual value and $\hat{y}_i$ is the predicted value.

\subsubsection{MSE - Mean Squared Error}
Mean Squared Error (MSE) calculates the average of the squared differences between predicted and actual values. By squaring the errors, MSE penalizes larger errors more severely than smaller ones. It is commonly used to measure the quality of a regression model and is useful for optimization because it is differentiable.

\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^n ( y_i - \hat{y}_i )^2
\]

\subsubsection{R² - Coefficient of Determination}
R-squared ($R^2$) measures the proportion of the variance in the dependent variable that is predictable from the independent variables. It indicates how well the model explains the variability of the outcome data. An $R^2$ value of 1 means perfect prediction, while 0 means the model does not explain any variability.

\[
R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n (y_i - \bar{y})^2}
\]

where $\bar{y}$ is the mean of the actual values.


\subsection{Logistic Regression}
Despite its name, Logistic Regression is used for binary classification tasks. It models the probability that an input belongs to a particular class using the logistic (sigmoid) function. The output is a probability value between 0 and 1.

\textbf{Example use case:} Predicting whether an email is spam or not.

\subsection{Decision Tree}
A Decision Tree is a non-parametric supervised learning method used for classification and regression. It splits the data into subsets based on feature values, creating a tree-like structure where each internal node represents a test on a feature, and each leaf node represents an output label.

\textbf{Example use case:} Classifying customer churn based on user behavior.

\subsection{K-Nearest Neighbors (KNN)}
KNN is an instance-based learning algorithm that classifies new data points based on the majority class of their \textit{k} nearest neighbors in the feature space. It is simple to implement and does not make assumptions about the underlying data distribution.

\textbf{Example use case:} Recommender systems or handwritten digit classification.

\subsection{Support Vector Machines (SVM)}
SVM is a powerful classification algorithm that works by finding the optimal hyperplane that best separates the data into different classes. It is effective in high-dimensional spaces and can be extended to non-linear classification using kernel functions.

\textbf{Example use case:} Face recognition or text classification.

\begin{flushleft}
Each model, except linear regression was trained on a portion of the dataset and evaluated using appropriate metrics such as accuracy, Jaccar index, F1-score and LogLoss (Which is exclusive for Logistic regression). Appendix~\ref{appendix:metrics}
\end{flushleft}

\section{Evaluation and Results}
The models were evaluated using a test dataset. The results are summarized below:

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
MAE & MSE & R² \\
\midrule
0.262167 & 0.118213 & 0.343821 \\

\bottomrule
\end{tabular}
\caption{Linear Regression Evaluration Metrics}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
Model & Accuracy & Jaccar Index & F1-Score & LogLoss \\
\midrule
Logistic Regression & 0.830534 & 0.453202 & 0.623729 & 6.108161 \\
Decision Tree       & 0.764885 & 0.376518 & 0.547059 \\
KNN                 & 0.810687 & 0.354167 & 0.523077 \\
SVM                 & 0.755725 & 0.418182 & 0.589744 \\
\bottomrule
\end{tabular}
\caption{Model Performance Comparison}
\end{table}
\newpage

\section{Conclusion}
\hspace*{1cm}This project demonstrates a complete classification workflow in machine learning, starting from data loading and preprocessing to model training and evaluation. By applying multiple classification algorithms to a real-world weather dataset, we gained practical insights into the behavior and performance of different models under the same conditions.

Among the models tested, \textbf{Logistic Regression} emerged as the best-performing approach in terms of both accuracy and robustness, offering a good balance between simplicity and predictive power. Other models like K-Nearest Neighbors and Support Vector Machines also showed competitive performance, although they may require more careful tuning and computational resources.

For future work, several enhancements could be explored to improve model performance. These include hyperparameter optimization using grid search or randomized search, as well as advanced ensemble methods such as Random Forest or Gradient Boosting. Additionally, incorporating feature selection techniques or domain-specific knowledge could further refine the predictive capabilities of the models.
\newpage

\appendix
\section{Appendix: Required libraries}
\begin{lstlisting}[language=Python]
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression
from sklearn import preprocessing
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import Decision
TreeClassifier
from sklearn import svm
from sklearn.metrics import jaccard_score
from sklearn.metrics import f1_score
from sklearn.metrics import log_loss
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score
import sklearn.metrics as metrics
\end{lstlisting}

\section{Appendix: Data preprocessing}
\label{appendix:preprocessing}
\begin{lstlisting}[language=Python]
#convert categorical variables to binary variables
df_sydney_processed = pd.get_dummies(data=df, columns=['RainToday', 'WindGustDir', 'WindDir9am', 'WindDir3pm'])

#replace the values of the 'RainTomorrow' column changing them from a categorical column to a binary column
df_sydney_processed.replace(['No', 'Yes'], [0,1], inplace=True)
\end{lstlisting}

\section{Appendix: Training and Test data}
\begin{lstlisting}[language=Python]
df_sydney_processed.drop('Date',axis=1,inplace=True)
df_sydney_processed = df_sydney_processed.astype(float)

#Define X (features) and Y
features = df_sydney_processed.drop(columns='RainTomorrow', axis=1)
Y = df_sydney_processed['RainTomorrow']
\end{lstlisting}

\section{Appendix: Create, train and predict with Linear regression}
\begin{lstlisting}[language=Python]
#Use the `train_test_split` function to split the `features` and `Y` dataframes with a `test_size` of `0.2` and the `random_state` set to `10`
x_train, x_test, y_train, y_test = train_test_split(features, Y, test_size=0.2, random_state=10)

#Create and train a Linear Regression model called LinearReg using the training data (`x_train`, `y_train`).
LinearReg = LinearRegression()
LinearReg.fit(x_train,y_train)

#Use the `predict` method on the testing data (`x_test`) and save it to the array `predictions`.
predictions = LinearReg.predict(x_test)
\end{lstlisting}

\section{Appendix: Linear Regression Evaluation metrics}
\label{appendix:LinearReg}
\begin{lstlisting}[language=Python]
from sklearn.metrics import mean_absolute_error, mean_squared_error
LinearRegression_MAE = mean_absolute_error(y_test, predictions)
LinearRegression_MSE = mean_squared_error(y_test, predictions)
LinearRegression_R2 = np.sqrt(LinearRegression_MSE)
\end{lstlisting}

\section{Appendix: Create others models}
\begin{lstlisting}[language=Python]
#Create model 

#KNN
KNN = KNeighborsClassifier(n_neighbors=4)

#Decision tree
Tree = DecisionTreeClassifier(random_state=101)

#Logistic Regression
LR = LogisticRegression(solver='liblinear')

#SVM
SVM = svm.SVC(class_weight='balanced')

\end{lstlisting}


\section{Appendix: Train and predict with others models}
\begin{lstlisting}[language=Python]
#model : KNN, Decision Tree, Logistic Regression, SVM

#Training
model.fit(x_train, y_train)

#Prediction
predictions = model.predict(x_test)
\end{lstlisting}
\newpage

\section{Appendix: Calculate values of each metrics}
\label{appendix:metrics}
\begin{lstlisting}[language=Python]
#model : KNN, Decision Tree, Logistic Regression(LR), SVM

model_Accuracy_Score = accuracy_score(y_test, predictions)
model_JaccardIndex = jaccard_score(y_test, predictions)
model_F1_Score = f1_score(y_test, predictions)

LR_Log_Loss = log_loss(y_test, predictions)
\end{lstlisting}

\end{document}
